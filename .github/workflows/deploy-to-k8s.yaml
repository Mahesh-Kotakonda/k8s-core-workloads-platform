name: Deploy to Kubernetes

on:
  repository_dispatch:
    types: [deploy-to-k8s]

env:
  FRONTEND_IMAGE: ${{ github.event.client_payload.frontend_image }}
  BACKEND_IMAGE:  ${{ github.event.client_payload.backend_image }}

jobs:

  # =====================================================
  # DEV + STAGE (AUTO DEPLOY)
  # =====================================================
  deploy-nonprod:
    runs-on: [self-hosted, deploy]

    env:
      NAMESPACES: "dev stage"

    steps:
      - uses: actions/checkout@v4

      # ---------------- VALIDATION ----------------
      - name: Validation
        run: |
          if [ -z "$FRONTEND_IMAGE" ] && [ -z "$BACKEND_IMAGE" ]; then
            echo "❌ No images provided."
            exit 1
          fi

          [[ -n "$FRONTEND_IMAGE" ]] && echo "DEPLOY_FRONTEND=true" >> $GITHUB_ENV
          [[ -n "$BACKEND_IMAGE"  ]] && echo "DEPLOY_BACKEND=true"  >> $GITHUB_ENV

      # ---------------- NAMESPACE CHECK ----------------
      - name: Ensure Namespaces
        run: |
          for ns in $NAMESPACES; do
            if ! kubectl get ns $ns >/dev/null 2>&1; then
              kubectl apply -f k8s/namespaces/$ns.yaml
              echo "FIRST_DEPLOY_$ns=true" >> $GITHUB_ENV
            else
              echo "FIRST_DEPLOY_$ns=false" >> $GITHUB_ENV
            fi
          done

      # ---------------- ECR SECRET ----------------
      - name: Create ECR Secret
        run: |
          ECR_PASSWORD=$(aws ecr get-login-password --region us-east-1)
          for ns in $NAMESPACES; do
            kubectl delete secret ecr-secret -n $ns --ignore-not-found
            kubectl create secret docker-registry ecr-secret \
              --namespace $ns \
              --docker-server=349070092739.dkr.ecr.us-east-1.amazonaws.com \
              --docker-username=AWS \
              --docker-password="$ECR_PASSWORD"
          done

      # ---------------- DEPLOY ----------------
      - name: Deploy Dev & Stage
        run: |
          for ns in $NAMESPACES; do

            echo "========== $ns =========="

            FIRST_DEPLOY=$(printenv FIRST_DEPLOY_$ns)

            if [ "$FIRST_DEPLOY" = "true" ]; then
              echo "First time deployment in $ns"

              kubectl apply -f k8s/backend/blue.yaml -n $ns
              kubectl apply -f k8s/backend/green.yaml -n $ns
              kubectl apply -f k8s/backend/service.yaml -n $ns

              kubectl apply -f k8s/frontend/canary-v1.yaml -n $ns
              kubectl apply -f k8s/frontend/canary-v2.yaml -n $ns
              kubectl apply -f k8s/frontend/service-clusterip.yaml -n $ns

              kubectl apply -f k8s/ingress/$ns.yaml -n $ns

              kubectl rollout status deployment/backend-blue -n $ns --timeout=180s
              kubectl rollout status deployment/frontend-v1 -n $ns --timeout=180s

            else
              echo "Existing deployment in $ns"

              # ----- BACKEND -----
              if [ "$DEPLOY_BACKEND" = "true" ]; then
                ACTIVE_COLOR=$(kubectl get svc backend-service -n $ns -o jsonpath='{.spec.selector.color}')
                TARGET_COLOR=$([ "$ACTIVE_COLOR" = "blue" ] && echo green || echo blue)

                kubectl set image deployment/backend-$TARGET_COLOR backend=$BACKEND_IMAGE -n $ns
                kubectl rollout status deployment/backend-$TARGET_COLOR -n $ns --timeout=180s

                kubectl patch svc backend-service -n $ns \
                  -p "{\"spec\":{\"selector\":{\"app\":\"backend\",\"color\":\"$TARGET_COLOR\"}}}"

                kubectl scale deployment backend-$ACTIVE_COLOR --replicas=0 -n $ns
              fi

              # ----- FRONTEND -----
              if [ "$DEPLOY_FRONTEND" = "true" ]; then
                kubectl set image deployment/frontend-v2 frontend=$FRONTEND_IMAGE -n $ns
                kubectl rollout status deployment/frontend-v2 -n $ns --timeout=180s

                kubectl scale deployment frontend-v2 --replicas=3 -n $ns
                kubectl scale deployment frontend-v1 --replicas=0 -n $ns
              fi
            fi
          done

      # ---------------- CLUSTER HEALTH ----------------
      - name: Verify Cluster Health (Dev & Stage)
        run: |
          kubectl get nodes
      
          NOT_READY=$(kubectl get nodes --no-headers | awk '$2 != "Ready" {print $1}')
          if [ -n "$NOT_READY" ]; then
            echo "❌ Nodes not ready"
            exit 1
          fi
      
          for ns in $NAMESPACES; do
            echo "Waiting for pods in $ns to become Ready..."
      
            kubectl wait --for=condition=Ready pod \
              --all \
              --namespace=$ns \
              --timeout=180s || exit 1
      
            echo "All pods Ready in $ns"
          done


      # ---------------- ALB HEALTH ----------------
      - name: Verify ALB & Target Group Health (Dev & Stage)
        run: |
          for ns in $NAMESPACES; do
          
            echo "Checking ALB for $ns"
          
            HOST=$(kubectl get ingress -n $ns \
              -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
          
            if [ -z "$HOST" ]; then
              echo "❌ No ALB hostname found in $ns"
              exit 1
            fi
          
            echo "ALB Hostname: $HOST"
          
            ALB_ARN=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?DNSName=='$HOST'].LoadBalancerArn" \
              --output text)
          
            echo "Waiting for ALB to become active..."
          
            for i in {1..30}; do
              STATE=$(aws elbv2 describe-load-balancers \
                --load-balancer-arns $ALB_ARN \
                --query "LoadBalancers[0].State.Code" \
                --output text)
          
              if [ "$STATE" = "active" ]; then
                echo "ALB is active in $ns"
                break
              fi
          
              echo "ALB state: $STATE — waiting..."
              sleep 10
            done
          
            if [ "$STATE" != "active" ]; then
              echo "❌ ALB did not become active in $ns"
              exit 1
            fi
          
            echo "Checking target group health..."
          
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --load-balancer-arn $ALB_ARN \
              --query "TargetGroups[*].TargetGroupArn" \
              --output text)
          
            for tg in $TG_ARNS; do
              for i in {1..20}; do
                HEALTH=$(aws elbv2 describe-target-health \
                  --target-group-arn $tg \
                  --query "TargetHealthDescriptions[*].TargetHealth.State" \
                  --output text)
          
                if [[ "$HEALTH" == *"healthy"* ]]; then
                  echo "Target group healthy"
                  break
                fi
          
                echo "Waiting for targets to become healthy..."
                sleep 10
              done
          
              if [[ "$HEALTH" != *"healthy"* ]]; then
                echo "❌ Target group unhealthy in $ns"
                exit 1
              fi
            done
          
          done



  # =====================================================
  # PROD (MANUAL APPROVAL REQUIRED)
  # =====================================================
  deploy-prod:
    needs: deploy-nonprod
    runs-on: [self-hosted, deploy]
    environment: prod

    steps:
      - uses: actions/checkout@v4

      - name: Validation
        run: |
          if [ -z "$FRONTEND_IMAGE" ] && [ -z "$BACKEND_IMAGE" ]; then
            echo "No images provided."
            exit 1
          fi

          [[ -n "$FRONTEND_IMAGE" ]] && echo "DEPLOY_FRONTEND=true" >> $GITHUB_ENV
          [[ -n "$BACKEND_IMAGE"  ]] && echo "DEPLOY_BACKEND=true"  >> $GITHUB_ENV

      - name: Ensure Prod Namespace
        run: |
          if ! kubectl get ns prod >/dev/null 2>&1; then
            kubectl apply -f k8s/namespaces/prod.yaml
            echo "FIRST_DEPLOY=true" >> $GITHUB_ENV
          else
            echo "FIRST_DEPLOY=false" >> $GITHUB_ENV
          fi

      - name: Create ECR Secret
        run: |
          ECR_PASSWORD=$(aws ecr get-login-password --region us-east-1)
          kubectl delete secret ecr-secret -n prod --ignore-not-found
          kubectl create secret docker-registry ecr-secret \
            --namespace prod \
            --docker-server=349070092739.dkr.ecr.us-east-1.amazonaws.com \
            --docker-username=AWS \
            --docker-password="$ECR_PASSWORD"

      - name: Deploy Prod
        run: |
          if [ "$FIRST_DEPLOY" = "true" ]; then
            kubectl apply -f k8s/backend/blue.yaml -n prod
            kubectl apply -f k8s/backend/green.yaml -n prod
            kubectl apply -f k8s/backend/service.yaml -n prod

            kubectl apply -f k8s/frontend/canary-v1.yaml -n prod
            kubectl apply -f k8s/frontend/canary-v2.yaml -n prod
            kubectl apply -f k8s/frontend/service-clusterip.yaml -n prod

            kubectl apply -f k8s/ingress/prod.yaml -n prod

          else
            if [ "$DEPLOY_BACKEND" = "true" ]; then
              ACTIVE_COLOR=$(kubectl get svc backend-service -n prod -o jsonpath='{.spec.selector.color}')
              TARGET_COLOR=$([ "$ACTIVE_COLOR" = "blue" ] && echo green || echo blue)

              kubectl set image deployment/backend-$TARGET_COLOR backend=$BACKEND_IMAGE -n prod
              kubectl rollout status deployment/backend-$TARGET_COLOR -n prod --timeout=180s

              kubectl patch svc backend-service -n prod \
                -p "{\"spec\":{\"selector\":{\"app\":\"backend\",\"color\":\"$TARGET_COLOR\"}}}"

              kubectl scale deployment backend-$ACTIVE_COLOR --replicas=0 -n prod
            fi

            if [ "$DEPLOY_FRONTEND" = "true" ]; then
              kubectl set image deployment/frontend-v2 frontend=$FRONTEND_IMAGE -n prod
              kubectl rollout status deployment/frontend-v2 -n prod --timeout=180s

              kubectl scale deployment frontend-v2 --replicas=3 -n prod
              kubectl scale deployment frontend-v1 --replicas=0 -n prod
            fi
          fi

      - name: Verify Prod Cluster & ALB
        run: |
          kubectl get nodes
          
          NOT_READY=$(kubectl get nodes --no-headers | awk '$2 != "Ready" {print $1}')
          if [ -n "$NOT_READY" ]; then
            echo "❌ Nodes not ready"
            exit 1
          fi
          
          echo "Waiting for prod pods to become Ready..."
          
          kubectl wait --for=condition=Ready pod \
            --all \
            --namespace=prod \
            --timeout=180s || exit 1
          
          echo "All prod pods Ready"

          echo "Checking ALB in prod..."
          
          HOST=$(kubectl get ingress -n prod \
            -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
          
          if [ -z "$HOST" ]; then
            echo "❌ No ALB hostname found in prod"
            exit 1
          fi
          
          echo "ALB Hostname: $HOST"
          
          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?DNSName=='$HOST'].LoadBalancerArn" \
            --output text)
          
          echo "Waiting for ALB to become active..."
          
          for i in {1..30}; do
            STATE=$(aws elbv2 describe-load-balancers \
              --load-balancer-arns $ALB_ARN \
              --query "LoadBalancers[0].State.Code" \
              --output text)
          
            if [ "$STATE" = "active" ]; then
              echo "ALB is active in prod"
              break
            fi
          
            echo "Current ALB state: $STATE — waiting..."
            sleep 10
          done
          
          if [ "$STATE" != "active" ]; then
            echo "❌ ALB did not become active in prod"
            exit 1
          fi
          echo "Checking target group health..."
          
          TG_ARNS=$(aws elbv2 describe-target-groups \
            --load-balancer-arn $ALB_ARN \
            --query "TargetGroups[*].TargetGroupArn" \
            --output text)
          
          for tg in $TG_ARNS; do
            for i in {1..20}; do
              HEALTH=$(aws elbv2 describe-target-health \
                --target-group-arn $tg \
                --query "TargetHealthDescriptions[*].TargetHealth.State" \
                --output text)
          
              if [[ "$HEALTH" == *"healthy"* ]]; then
                echo "Target group healthy"
                break
              fi
          
              echo "Waiting for targets to become healthy..."
              sleep 10
            done
          
            if [[ "$HEALTH" != *"healthy"* ]]; then
              echo "❌ Target group unhealthy in prod"
              exit 1
            fi
          done
